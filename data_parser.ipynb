{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from akashiwo_data_scrape import *\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speacies id\n",
    "spicies_ids = [45, 3]\n",
    "spicies_names = [\"Diaton.csv\", \"shtonela.csv\"]\n",
    "\n",
    "for id, names in zip(spicies_ids, spicies_names):\n",
    "    pull_data(id, \"2011/5/01\", \"2023/10/11\", names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "colum_names = ['lat', 'lng', 'speciesValueAM', 'speciesValuePM', \n",
    "               'saisuiValueAM', 'saisuiValuePM', 'speciesNameKana', \n",
    "               'maxvalue', 'speciesId', 'icon_size']\n",
    "\n",
    "for name in spicies_names:\n",
    "    remove_colum(colum_names, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"shtonela.csv\")\n",
    "pointIds = data[\"pointId\"]\n",
    "gatherYMDs = data[\"gatherYMD\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable int object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb#X55sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(html_data\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mhtml.parser\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb#X55sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m tables \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39m'\u001b[39m\u001b[39mtable\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb#X55sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m times_to_duplicate, main_df \u001b[39m=\u001b[39m parse_main_table(tables[\u001b[39m1\u001b[39m], headers_to_skip_main, main_df)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb#X55sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m coordinate_df \u001b[39m=\u001b[39m parse_coordinate_table(tables[\u001b[39m0\u001b[39m], coordinate_df, times_to_duplicate)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lukakirtadze/Desktop/deep_learning_with_python/data_parser.ipynb#X55sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m(i \u001b[39m==\u001b[39m \u001b[39m10\u001b[39m): \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable int object"
     ]
    }
   ],
   "source": [
    "main_data = {}\n",
    "coordinate_data = {}\n",
    "main_df = pd.DataFrame(main_data)\n",
    "coordinate_df = pd.DataFrame(coordinate_data)\n",
    "headers_to_skip_main = [\"確定値／速報値\", \"事業・調査名\"]\n",
    "i = 0\n",
    "for pointId, gatherYMD in zip(pointIds, gatherYMDs):\n",
    "    html_data = requests.get(f\"https://akashiwo.jp/private/akashiwoListInit.php?qpoint_id={str(pointId)}&qspecies_id=3&qgather_ymd_s=&qgather_ymd_e={str(gatherYMD)}\")\n",
    "    html_data.encoding = 'utf-8'\n",
    "    soup = BeautifulSoup(html_data.text, 'html.parser')\n",
    "    tables = soup.find_all('table')\n",
    "    times_to_duplicate, main_df = parse_main_table(tables[1], headers_to_skip_main, main_df)\n",
    "    coordinate_df = parse_coordinate_table(tables[0], coordinate_df, times_to_duplicate)\n",
    "    if(i == 10): break\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = coordinate_df.merge(main_df, left_index=True, right_index=True)\n",
    "combined_data.to_csv(\"shatonela.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
